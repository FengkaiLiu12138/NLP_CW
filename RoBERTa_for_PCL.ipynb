{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:42:43.512698Z",
     "start_time": "2025-02-21T11:42:32.783473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, \\\n",
    "    pipeline\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from transformers import logging\n",
    "import os\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Download NLTK data (WordNet for synonyms, stopwords list)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# Data augmentation: Synonym Replacement\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "4bc0281ff21894f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yiqin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yiqin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:42:47.669843Z",
     "start_time": "2025-02-21T11:42:47.637471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./data/dontpatronizeme_pcl.tsv', sep='\\t', header=None,\n",
    "                 names=[\"par_id\", \"art_id\", \"keyword\", \"country\", \"paragraph\", \"orig_label\"], skiprows=4)\n",
    "df['label'] = df['orig_label'].apply(lambda x: 1 if x >= 2 else 0)\n",
    "\n",
    "df.info"
   ],
   "id": "3c877c4330cce04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        par_id      art_id     keyword country  \\\n",
       "0           1  @@24942188    hopeless      ph   \n",
       "1           2  @@21968160     migrant      gh   \n",
       "2           3  @@16584954   immigrant      ie   \n",
       "3           4   @@7811231    disabled      nz   \n",
       "4           5   @@1494111     refugee      ca   \n",
       "...       ...         ...         ...     ...   \n",
       "10464   10465  @@14297363       women      lk   \n",
       "10465   10466  @@70091353  vulnerable      ph   \n",
       "10466   10467  @@20282330     in-need      ng   \n",
       "10467   10468  @@16753236    hopeless      in   \n",
       "10468   10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                               paragraph  orig_label  label  \n",
       "0      We 're living in times of absolute insanity , ...           0      0  \n",
       "1      In Libya today , there are countless number of...           0      0  \n",
       "2      White House press secretary Sean Spicer said t...           0      0  \n",
       "3      Council customers only signs would be displaye...           0      0  \n",
       "4      \" Just like we received migrants fleeing El Sa...           0      0  \n",
       "...                                                  ...         ...    ...  \n",
       "10464  Sri Lankan norms and culture inhibit women fro...           1      0  \n",
       "10465  He added that the AFP will continue to bank on...           0      0  \n",
       "10466  \" She has one huge platform , and information ...           3      1  \n",
       "10467  \" Anja Ringgren Loven I ca n't find a word to ...           4      1  \n",
       "10468  \" Guinness World Record of 540lbs of 7-layer m...           3      1  \n",
       "\n",
       "[10469 rows x 7 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:42:54.247203Z",
     "start_time": "2025-02-21T11:42:48.935938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    # Identify candidate words (non-stopwords, alphabetic only)\n",
    "    candidates = [i for i, w in enumerate(words) if w.lower() not in stop_words and w.isalpha()]\n",
    "    random.shuffle(candidates)\n",
    "    replaced = 0\n",
    "    for idx in candidates:\n",
    "        if replaced >= n:\n",
    "            break\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(words[idx]):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name().lower() != words[idx].lower():\n",
    "                    synonyms.add(lemma.name().replace('_', ' '))\n",
    "        if synonyms:\n",
    "            new_words[idx] = random.choice(list(synonyms))\n",
    "            replaced += 1\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "translator_en_to_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=-1)\n",
    "translator_es_to_en = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\", device=-1)\n",
    "\n",
    "def back_translate(sentence):\n",
    "    try:\n",
    "        es_text = translator_en_to_es(sentence, max_length=MAX_LENGTH)[0]['translation_text']\n",
    "        back_text = translator_es_to_en(es_text, max_length=MAX_LENGTH)[0]['translation_text']\n",
    "        return back_text\n",
    "    except Exception as e:\n",
    "        return sentence  # fallback to original if translation fails\n",
    "\n",
    "\n",
    "# Augment training data (apply to minority class samples)\n",
    "def augment_data(df):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    for _, row in df[df['label'] == 1].iterrows():\n",
    "        text = row['paragraph']\n",
    "        # Synonym replacement augmentation\n",
    "        aug_text1 = synonym_replacement(text, n=1)\n",
    "        # Back-translation augmentation\n",
    "        aug_text2 = back_translate(text)\n",
    "        augmented_texts.extend([aug_text1, aug_text2])\n",
    "        augmented_labels.extend([1, 1])\n",
    "    aug_df = pd.DataFrame({'paragraph': augmented_texts, 'label': augmented_labels})\n",
    "    # Combine augmented samples with original data\n",
    "    return pd.concat([df, aug_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Oversample minority class to balance the dataset\n",
    "def oversample_data(df):\n",
    "    df_majority = df[df['label'] == 0]\n",
    "    df_minority = df[df['label'] == 1]\n",
    "    if len(df_minority) == 0 or len(df_majority) == 0:\n",
    "        return df\n",
    "    if len(df_minority) < len(df_majority):\n",
    "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=0)\n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled], ignore_index=True)\n",
    "    else:\n",
    "        df_majority_upsampled = resample(df_majority, replace=True, n_samples=len(df_minority), random_state=0)\n",
    "        df_balanced = pd.concat([df_minority, df_majority_upsampled], ignore_index=True)\n",
    "    # Shuffle after oversampling\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    return df_balanced"
   ],
   "id": "8232ad544cc6916d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:42:55.781884Z",
     "start_time": "2025-02-21T11:42:55.764803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=0)\n",
    "test_df, val_df = train_test_split(test_val_df, test_size=0.5, stratify=test_val_df['label'], random_state=0)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.info())"
   ],
   "id": "aa5e3249d75aef6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   par_id      art_id        keyword country  \\\n",
      "0    1046  @@19202846      immigrant      bd   \n",
      "1    3020  @@23325397  poor-families      hk   \n",
      "2    4243   @@8774436        in-need      gb   \n",
      "3    7858   @@8634994     vulnerable      ng   \n",
      "4    2549   @@2688379          women      au   \n",
      "\n",
      "                                           paragraph  orig_label  label  \n",
      "0  Many advocates for immigrants say they are hop...           0      0  \n",
      "1  Dozens of children and parents from poor famil...           0      0  \n",
      "2  No one was injured in the incident although th...           0      0  \n",
      "3  \" Owing to this drought and the on-going water...           0      0  \n",
      "4  \" It is only a matter of luck that Ms Stevens ...           1      0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8375 entries, 0 to 8374\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   par_id      8375 non-null   int64 \n",
      " 1   art_id      8375 non-null   object\n",
      " 2   keyword     8375 non-null   object\n",
      " 3   country     8375 non-null   object\n",
      " 4   paragraph   8374 non-null   object\n",
      " 5   orig_label  8375 non-null   int64 \n",
      " 6   label       8375 non-null   int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 458.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:44:27.095215Z",
     "start_time": "2025-02-21T11:44:27.049580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = train_df.dropna(subset=['paragraph'])\n",
    "train_df['paragraph'] = train_df['paragraph'].astype(str)\n",
    "\n",
    "try:\n",
    "\n",
    "    # If you've already saved train_augmented_oversampled.csv\n",
    "    train_df_final = pd.read_csv(\"./example_training_data/train_augmented_oversampled.csv\")\n",
    "    val_df = pd.read_csv(\"./example_training_data/val.csv\")\n",
    "    test_df = pd.read_csv(\"./example_training_data/test.csv\")\n",
    "except:\n",
    "\n",
    "    # Augment and balance training data\n",
    "    train_df_aug = augment_data(train_df)\n",
    "    train_df_final = oversample_data(train_df_aug)\n",
    "    for idx, row in train_df.iterrows():\n",
    "        if not isinstance(row['paragraph'], str):\n",
    "            print(f\"Row {idx} has invalid type: {type(row['paragraph'])}, value = {row['paragraph']}\")\n",
    "\n",
    "    train_df_final.to_csv(\"./example_training_data/train_augmented_oversampled.csv\", index=False)\n",
    "    val_df.to_csv(\"./example_training_data/val.csv\", index=False)\n",
    "    test_df.to_csv(\"./example_training_data/test.csv\", index=False)\n",
    "\n",
    "train_df_final.info\n"
   ],
   "id": "15266e0c0a0398b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         par_id      art_id    keyword country  \\\n",
       "0          NaN         NaN        NaN     NaN   \n",
       "1      10269.0  @@15357075   homeless      gb   \n",
       "2          NaN         NaN        NaN     NaN   \n",
       "3       2214.0  @@24216252  immigrant      ie   \n",
       "4          NaN         NaN        NaN     NaN   \n",
       "...        ...         ...        ...     ...   \n",
       "15155      NaN         NaN        NaN     NaN   \n",
       "15156   7317.0  @@20518391    in-need      in   \n",
       "15157      NaN         NaN        NaN     NaN   \n",
       "15158      NaN         NaN        NaN     NaN   \n",
       "15159   9380.0  @@21676447  immigrant      hk   \n",
       "\n",
       "                                               paragraph  orig_label  label  \n",
       "0      Watching poor families in England writhe in pa...         NaN      1  \n",
       "1      Veterans left on scrapheap : The homeless plig...         4.0      1  \n",
       "2      After a tragic event during his previous life ...         NaN      1  \n",
       "3      Labour said it was \" truly shocking \" that the...         0.0      0  \n",
       "4      He may well have depression and this may have ...         NaN      1  \n",
       "...                                                  ...         ...    ...  \n",
       "15155  Whenever the term ' disabled ' is mentioned , ...         NaN      1  \n",
       "15156  Founded with only 13 members initially in Dece...         0.0      0  \n",
       "15157  Together they will bring more than 70 years of...         NaN      1  \n",
       "15158  \" Every family which qualifies for the program...         NaN      1  \n",
       "15159  The simulation began with officers taking 11 i...         0.0      0  \n",
       "\n",
       "[15160 rows x 7 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:45:01.509469Z",
     "start_time": "2025-02-21T11:45:01.502834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "PATIENCE = 3\n",
    "WARMUP_PROPORTION = 0.1\n",
    "LR_CANDIDATES = [1e-5, 2e-5, 5e-5]\n",
    "ENSEMBLE_SEEDS = [42, 52, 62]"
   ],
   "id": "bd02f7876324fdb5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T11:45:03.604094Z",
     "start_time": "2025-02-21T11:45:03.592576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoader for a given DataFrame\n",
    "def build_dataloader(df, tokenizer, batch_size, shuffle=False):\n",
    "    texts = list(df['paragraph'])\n",
    "    encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    if 'label' in df.columns:\n",
    "        labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "        dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "    else:\n",
    "        dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "\n",
    "# Training function for one model\n",
    "def train_model(train_loader, val_loader, learning_rate, num_epochs=NUM_EPOCHS, patience=PATIENCE, seed=42):\n",
    "    set_seed(seed)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    warmup_steps = int(WARMUP_PROPORTION * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=total_steps)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    best_f1 = 0.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            # Forward pass (with mixed precision)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            # Backward pass and optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {total_loss / len(train_loader):.4f} - Val F1: {val_f1:.4f}\")\n",
    "        # Check for improvement\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best weights and return model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model, best_f1"
   ],
   "id": "e3a213eb19442fa4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:05:43.709216Z",
     "start_time": "2025-02-21T12:05:40.602063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and create DataLoaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_loader = build_dataloader(train_df, tokenizer, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = build_dataloader(val_df, tokenizer, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = build_dataloader(test_df, tokenizer, batch_size=BATCH_SIZE,\n",
    "                               shuffle=False) if test_df is not None else None\n",
    "\n",
    "try:\n",
    "    ensemble_models = []\n",
    "    n_models = 3  # how many you saved\n",
    "    for i in range(n_models):\n",
    "        load_dir = f'ensemble_model_{i}'\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(load_dir)\n",
    "        model.to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(load_dir)\n",
    "        ensemble_models.append(model)\n",
    "        print(\"Loaded model from\", load_dir)\n",
    "except:\n",
    "\n",
    "    # Hyperparameter tuning (learning rate)\n",
    "    best_lr = None\n",
    "    best_val_f1 = 0.0\n",
    "    print(\"Hyperparameter tuning for learning rate...\")\n",
    "    for lr in LR_CANDIDATES:\n",
    "        print(f\"Training with lr={lr}\")\n",
    "        model, val_f1 = train_model(train_loader, val_loader, learning_rate=lr, num_epochs=3, patience=2, seed=42)\n",
    "        print(f\"Val F1 = {val_f1:.4f} for lr={lr}\")\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_lr = lr\n",
    "        # Free GPU memory before next trial\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Best LR: {best_lr}, Val F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Train ensemble models with best hyperparameters\n",
    "    ensemble_models = []\n",
    "    for seed in ENSEMBLE_SEEDS:\n",
    "        print(f\"Training model with seed {seed} (lr={best_lr})...\")\n",
    "        model, val_f1 = train_model(train_loader, val_loader, learning_rate=best_lr, num_epochs=NUM_EPOCHS,\n",
    "                                    patience=PATIENCE, seed=seed)\n",
    "        print(f\"Model seed {seed} Val F1 = {val_f1:.4f}\")\n",
    "        ensemble_models.append(model)\n",
    "\n",
    "    for i, model in enumerate(ensemble_models):\n",
    "        save_dir = f'ensemble_model_{i}'\n",
    "        # model is a standard huggingface model derived from AutoModelForSequenceClassification\n",
    "        model.save_pretrained(save_dir)\n",
    "        # Also save tokenizer if you want a self-contained folder\n",
    "        tokenizer.save_pretrained(save_dir)"
   ],
   "id": "82d8adbbcf75daa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ensemble_model_0\n",
      "Loaded model from ensemble_model_1\n",
      "Loaded model from ensemble_model_2\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:08:13.879678Z",
     "start_time": "2025-02-21T12:07:25.628290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble predictions on validation set (majority voting)\n",
    "all_dev_preds = []\n",
    "for model in ensemble_models:\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # val_loader is a DataLoader object\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            preds.extend(batch_preds)\n",
    "    all_dev_preds.append(preds)\n",
    "\n",
    "# Transpose and vote\n",
    "all_dev_preds = np.array(all_dev_preds)  # shape: (num_models, num_examples)\n",
    "ensemble_dev_preds = []\n",
    "for j in range(all_dev_preds.shape[1]):\n",
    "    votes = np.sum(all_dev_preds[:, j])\n",
    "    ensemble_dev_preds.append(1 if votes > len(ensemble_models) / 2 else 0)\n",
    "\n",
    "# We need the original validation DataFrame for labels and paragraph IDs\n",
    "val_labels = val_df['label'].values  # Use val_df, not val_loader\n",
    "dev_f1 = f1_score(val_labels, ensemble_dev_preds, average='binary')\n",
    "print(f\"Ensemble Validation F1 = {dev_f1:.4f}\")\n",
    "\n",
    "# Ensemble predictions on test set\n",
    "ensemble_test_preds = []\n",
    "if test_loader is not None:\n",
    "    all_test_preds = []\n",
    "    for model in ensemble_models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                preds.extend(batch_preds)\n",
    "        all_test_preds.append(preds)\n",
    "    all_test_preds = np.array(all_test_preds)\n",
    "    for j in range(all_test_preds.shape[1]):\n",
    "        votes = np.sum(all_test_preds[:, j])\n",
    "        ensemble_test_preds.append(1 if votes > len(ensemble_models) / 2 else 0)\n",
    "\n",
    "# Save predictions to files using the original DataFrames\n",
    "with open(\"predictions_dev.txt\", \"w\") as f:\n",
    "    for pid, pred in zip(val_df['par_id'], ensemble_dev_preds):\n",
    "        f.write(f\"{pid}\\t{pred}\\n\")\n",
    "\n",
    "if test_df is not None:\n",
    "    with open(\"predictions_test.txt\", \"w\") as f:\n",
    "        for pid, pred in zip(test_df['par_id'], ensemble_test_preds):\n",
    "            f.write(f\"{pid}\\t{pred}\\n\")\n",
    "\n",
    "print(\"Prediction files generated: predictions_dev.txt, predictions_test.txt\")"
   ],
   "id": "118be6b2a5a2edeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation F1 = 0.6452\n",
      "Prediction files generated: predictions_dev.txt, predictions_test.txt\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
